{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AfxuTWXTy_lj"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import h5py\n",
        "import numpy as np\n",
        "import json\n",
        "import urllib.request\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from IPython.display import Image, display, clear_output\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "%matplotlib inline\n",
        "sns.set_style('whitegrid')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras import optimizers\n",
        "from keras.applications.vgg16 import VGG16\n",
        "from keras.models import Sequential, load_model, Model\n",
        "from keras.layers import Conv2D, MaxPooling2D, ZeroPadding2D, Activation, Dropout, Flatten, Dense, Input\n",
        "from keras.regularizers import l2, l1\n",
        "from keras.utils.np_utils import to_categorical\n",
        "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
        "from keras.callbacks import ModelCheckpoint, History\n",
        "from keras import backend as K\n",
        "from keras.utils.data_utils import get_file"
      ],
      "metadata": {
        "id": "f6lVCLW_zCYj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_metrics(hist, stop=50):\n",
        "    fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(10,4))\n",
        "                            \n",
        "    axes = axes.flatten()\n",
        "\n",
        "    axes[0].plot(range(stop), hist['acc'], label='Training', color='#FF533D')\n",
        "    axes[0].plot(range(stop), hist['val_acc'], label='Validation', color='#03507E')\n",
        "    axes[0].set_title('Accuracy')\n",
        "    axes[0].set_ylabel('Accuracy')\n",
        "    axes[0].set_xlabel('Epoch')\n",
        "    axes[0].legend(loc='lower right')\n",
        "                             \n",
        "    axes[1].plot(range(stop), hist['loss'], label='Training', color='#FF533D')\n",
        "    axes[1].plot(range(stop), hist['val_loss'], label='Validation', color='#03507E')\n",
        "    axes[1].set_title('Loss')\n",
        "    axes[1].set_ylabel('Loss')\n",
        "    axes[1].set_xlabel('Epoch')\n",
        "    axes[1].legend(loc='upper right')\n",
        "                             \n",
        "    plt.tight_layout();\n",
        "    \n",
        "    print(\"Best Model:\") \n",
        "    print_best_model_results(hist)"
      ],
      "metadata": {
        "id": "vCKFSGOyzCaI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_acc_metrics(hist1, hist2, stop=50):\n",
        "    fig, axes = plt.subplots(nrows=2, ncols=1, figsize=(4.25,6))\n",
        "                            \n",
        "    axes = axes.flatten()\n",
        "    \n",
        "    axes[0].plot(range(stop), hist1['acc'], label='Training', color='#FF533D')\n",
        "    axes[0].plot(range(stop), hist1['val_acc'], label='Validation', color='#03507E')\n",
        "    axes[0].set_title('Training')\n",
        "    axes[0].set_ylabel('Accuracy')\n",
        "    axes[0].set_xlabel('Epoch')\n",
        "    axes[0].legend(loc='lower right')\n",
        "                             \n",
        "    axes[1].plot(range(stop), hist2['acc'], label='Training', color='#FF533D')\n",
        "    axes[1].plot(range(stop), hist2['val_acc'], label='Validation', color='#03507E')\n",
        "    axes[1].set_title('Fine-tuning')\n",
        "    axes[1].set_ylabel('Accuracy')\n",
        "    axes[1].set_xlabel('Epoch')\n",
        "    axes[1].legend(loc='lower right')\n",
        "                             \n",
        "    plt.tight_layout();"
      ],
      "metadata": {
        "id": "Y47hQHaozCeJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_best_model_results(model_hist):\n",
        "    best_epoch = np.argmax(model_hist['val_acc'])\n",
        "    print('epoch:', best_epoch+1, \\\n",
        "    ', val_acc:', model_hist['val_acc'][best_epoch], \\\n",
        "    ', val_loss:', model_hist['val_loss'][best_epoch])"
      ],
      "metadata": {
        "id": "ftLZayAWzCs-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_bottleneck_features():\n",
        "    datagen = ImageDataGenerator(rescale=1./255)\n",
        "    \n",
        "    model = VGG16(include_top=False, weights='imagenet')\n",
        "    \n",
        "    generator = datagen.flow_from_directory(train_data_dir, target_size=(img_width, img_height), batch_size=batch_size, class_mode=None, shuffle=False)\n",
        "    bottleneck_features_train = model.predict_generator(generator, nb_train_samples // batch_size)\n",
        "    np.save(location+'/bottleneck_features_train.npy', bottleneck_features_train)\n",
        "    \n",
        "    generator = datagen.flow_from_directory(validation_data_dir, target_size=(img_width, img_height), batch_size=batch_size, class_mode=None, shuffle=False)\n",
        "    bottleneck_features_validation = model.predict_generator(generator, nb_validation_samples // batch_size)\n",
        "    np.save(location+'/bottleneck_features_validation.npy', bottleneck_features_validation)"
      ],
      "metadata": {
        "id": "wzlDg2D70H6i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_top_model():\n",
        "    train_data = np.load(location+'/bottleneck_features_train.npy')\n",
        "    train_labels = np.array([0] * (nb_train_samples // 2) + [1] * (nb_train_samples // 2))\n",
        "    \n",
        "    validation_data = np.load(location+'/bottleneck_features_validation.npy')\n",
        "    validation_labels = np.array([0] * (nb_validation_samples // 2) + [1] * (nb_validation_samples // 2))\n",
        "    \n",
        "    model = Sequential()\n",
        "    model.add(Flatten(input_shape=train_data.shape[1:]))\n",
        "    model.add(Dense(256,activation='relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(1,activation='sigmoid'))\n",
        "    model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    \n",
        "    checkpoint = ModelCheckpoint(top_model_weights_path, monitor='val_acc', verbose=1, save_best_only=True, save_weights_only=True, mode='auto')\n",
        "    \n",
        "    fit = model.fit(train_data, train_labels, epochs=epochs, batch_size=batch_size,validation_data=(validation_data,validation_labels), callbacks=[checkpoint])\n",
        "    \n",
        "    with open(location+'/top_history.txt', 'w') as f:\n",
        "        json.dump(fit.history, f) \n",
        "        \n",
        "    return model, fit.history"
      ],
      "metadata": {
        "id": "7p8rDzyz0H85"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def finetune_binary_model():\n",
        "    base_model = VGG16(weights='imagenet', include_top=False, input_shape=(256,256,3))\n",
        "    print(\"Model loaded.\")\n",
        "    \n",
        "    top_model = Sequential()\n",
        "    top_model.add(Flatten(input_shape=base_model.output_shape[1:]))\n",
        "    top_model.add(Dense(256, activation='relu'))\n",
        "    top_model.add(Dropout(0.5))\n",
        "    top_model.add(Dense(1, activation='sigmoid'))\n",
        "    \n",
        "    top_model.load_weights(top_model_weights_path)\n",
        "    \n",
        "    model = Model(inputs=base_model.input, outputs=top_model(base_model.output))\n",
        "    \n",
        "    for layer in model.layers[:25]:\n",
        "        layer.trainable = False\n",
        "    \n",
        "    model.compile(loss='binary_crossentropy', optimizer=optimizers.SGD(lr=1e-4, momentum=0.9), metrics=['accuracy'])\n",
        "    \n",
        "    train_datagen = ImageDataGenerator(rescale = 1./255, zoom_range=0.2, shear_range=0.2, horizontal_flip=True)\n",
        "    \n",
        "    test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "    \n",
        "    train_generator = train_datagen.flow_from_directory(train_data_dir, target_size=(img_height, img_width), batch_size=batch_size, class_mode='binary')\n",
        "    \n",
        "    validation_generator = test_datagen.flow_from_directory(validation_data_dir, target_size=(img_height, img_width), batch_size=batch_size, class_mode='binary')\n",
        "    \n",
        "    checkpoint = ModelCheckpoint(fine_tuned_model_path, monitor='val_acc', verbose=1, save_best_only=True, save_weights_only=False, mode='auto')\n",
        "    \n",
        "    fit = model.fit_generator(train_generator, steps_per_epoch=nb_train_samples//batch_size, epochs=epochs, validation_data=validation_generator, validation_steps=nb_validation_samples//batch_size, verbose=1, callbacks=[checkpoint])\n",
        "    \n",
        "    with open(location+'/ft_history.txt', 'w') as f:\n",
        "        json.dump(fit.history, f)\n",
        "        \n",
        "    return model, fit.history"
      ],
      "metadata": {
        "id": "k7ztI7kQ0M5B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_binary_model(model, directory, labels):\n",
        "    datagen = ImageDataGenerator(rescale=1./255)\n",
        "    \n",
        "    generator = datagen.flow_from_directory(directory, target_size=(img_height,img_width), batch_size=batch_size, class_mode='binary', shuffle=False)\n",
        "    \n",
        "    predictions = model.predict_generator(generator, len(labels))\n",
        "    \n",
        "    pred_labels = [0 if i<0.5 else 1 for i in predictions]\n",
        "    \n",
        "    print('')\n",
        "    print(classification_report(validation_labels, pred_labels))\n",
        "    print('')\n",
        "    cm = confusion_matrix(validation_labels, pred_labels)\n",
        "    return cm"
      ],
      "metadata": {
        "id": "XHMsJLX-0M96"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "location = 'data2'\n",
        "top_model_weights_path = location+'/top_model_weights.h5'\n",
        "fine_tuned_model_path = location+'/ft_model.h5'\n",
        "\n",
        "train_data_dir = location+'/training'\n",
        "validation_data_dir = location+'/validation'\n",
        "train_samples = [len(os.listdir(train_data_dir+'/'+i)) for i in sorted(os.listdir(train_data_dir))]\n",
        "nb_train_samples = 1824\n",
        "validation_samples = [len(os.listdir(validation_data_dir+'/'+i)) for i in sorted(os.listdir(validation_data_dir))]\n",
        "nb_validation_samples = 448\n",
        "\n",
        "img_width, img_height = 256,256\n",
        "epochs = 50\n",
        "batch_size = 16\n",
        "save_bottleneck_features()\n"
      ],
      "metadata": {
        "id": "1H8iPuen0PuL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d2_model1, d2_history1 = train_top_model()"
      ],
      "metadata": {
        "id": "8FthHH570VfS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_metrics(d2_history1)"
      ],
      "metadata": {
        "id": "OHXPd0nl0Pw_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ft_model, ft_history = finetune_binary_model()\n",
        "plot_metrics(ft_history)"
      ],
      "metadata": {
        "id": "QAJfc62O0dHg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ft_model = load_model(location+'/ft_model.h5')"
      ],
      "metadata": {
        "id": "Y_Fmtrh20f-4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('data1a/top_history.txt') as f:    \n",
        "    top_history = json.load(f)\n",
        "with open('data1a/ft_history.txt') as f:    \n",
        "    ft_history = json.load(f)\n",
        "plot_acc_metrics(top_history, ft_history)\n",
        "validation_labels = np.array([0] * (nb_validation_samples // 2) + [1] * (nb_validation_samples // 2))\n",
        "cm = evaluate_binary_model(ft_model, validation_data_dir, validation_labels)"
      ],
      "metadata": {
        "id": "_-DAmAzV0kAA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "heatmap_laebls = ['Damaged', 'Whole']\n",
        "sns.heatmap(cm, annot=True, annot_kws={\"size\":16}, fmt='g', cmap='OrRd', xticklabels=heatmap_labels, yticklabels=heatmap_labels)\n",
        "sns.heatmap(cm, annot=Ture, annot_kws={\"size\":16}, fmt='g', cmap='Blues', xticklabels=heatmap_labels, yticklabels=heatmap_labels)"
      ],
      "metadata": {
        "id": "Tr0H6ET60pSZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pipe2(image_path, model):\n",
        "    urllib.request.urlretrieve(image_path, 'save.jpg')\n",
        "    img = load_img('save.jpg', target_size=(256,256))\n",
        "    x = img_to_array(img)\n",
        "    x = x.reshape((1,) + x.shape)/255\n",
        "    pred = model.predict(x)\n",
        "    print(\"Validating that damage exists....\")\n",
        "    print(pred)\n",
        "    if(pred[0][0]<=0.5):\n",
        "        print(\"Validation complete - proceed to location and severity determination\")\n",
        "    else:\n",
        "        print (\"Are you sure that your car is damaged? Please submit another picture of the damage.\")\n",
        "        print (\"Hint: Try zooming in/out, using a different angle or different lighting\")      \n",
        "Image('http://3.bp.blogspot.com/-PrRY9XxCqYQ/UDNutnMI7LI/AAAAAAAABdw/UGygghh-hRA/s1600/Bumper+scuff.JPG')"
      ],
      "metadata": {
        "id": "tUdKcrHg0sBB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipe2('https://i.ytimg.com/vi/4oV1klVPogY/maxresdefault.jpg', ft_model)"
      ],
      "metadata": {
        "id": "w8cTyUXd0uX6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Image('http://blog.automart.co.za/wp-content/uploads/2014/09/Accident_Damaged_Car.png')"
      ],
      "metadata": {
        "id": "xfHjEC0e0v5Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipe2('http://blog.automart.co.za/wp-content/uploads/2014/09/Accident_Damaged_Car.png', ft_model)\n"
      ],
      "metadata": {
        "id": "Tth8zqdh0yLI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}